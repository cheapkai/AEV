{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "4tbIXbUojRw6"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cheapkai/AEV/blob/master/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ak52g5n7izog"
      },
      "source": [
        "!pip install pydicom\n",
        "!pip install opencv-python\n",
        "!pip install pillow # optional \n",
        "\n",
        "!pip install pandas "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Orx9SLpxe33J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJv1M-OrjP9V"
      },
      "source": [
        "import pydicom as dicom\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "import PIL # optional\n",
        "import pandas as pd\n",
        "import csv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNUc8rw1jXVJ"
      },
      "source": [
        "# make it True if you want in PNG format\n",
        "PNG = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2J0XhX_jjzb"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y330AkMckBF6"
      },
      "source": [
        "!mkdir jpegfromdicom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3mPloqokGl0"
      },
      "source": [
        "from google.colab import drive\n",
        "#from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JfctV7vfzCl"
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oab7j6Yte5vf"
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3hqFhs4f1HI"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "569SpS-NctF_"
      },
      "source": [
        "!rm 'extract (1).py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbaH4GaBIl3Z"
      },
      "source": [
        "!ls /content/gdrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2t_F5bH6FM7"
      },
      "source": [
        "%cd /content/gdrive/MyDrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei6W4fvA6OwO"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRK_cmg66UQ4"
      },
      "source": [
        "!cd ./gdrive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzouQC7L7e-1"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVAIO8HM7rne"
      },
      "source": [
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIDsbTL2GGqE"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbOMu_pXk5q-"
      },
      "source": [
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eb0duFbElGxi"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-4KPkgs8VZ5"
      },
      "source": [
        "!cp ./manifest-RbPGRCVv7392292744865323559.zip ../../."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2k6ss11rm7Jc"
      },
      "source": [
        "%cd /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jq2n7bMHRvI"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7eq808LHml9"
      },
      "source": [
        "drive.flush_and_unmount()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CqwGNzfUtsy"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qakvMdNPLKQl"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u9T5IBLMW61"
      },
      "source": [
        "!ls -lh manifest-RbPGRCVv7392292744865323559/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnpzJpD0N0uV"
      },
      "source": [
        "%cd manifest-RbPGRCVv7392292744865323559/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AR_u85OnljV4"
      },
      "source": [
        "%cd Breast-MRI-NACT-Pilot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3EptH7ols_R"
      },
      "source": [
        "%cd /content/manifest-RbPGRCVv7392292744865323559/Breast-MRI-NACT-Pilot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isEv96Q8l5Ai"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF3UxD-kHmaU"
      },
      "source": [
        "!rm -rf ./sample_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItAN4v_RMFgw"
      },
      "source": [
        "!rm -rf manifest-RbPGRCVv7392292744865323559"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAauT7SGfCSl"
      },
      "source": [
        "!rm PAK.npy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBBcfbI9WVeB"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6JwuGvk8l1x"
      },
      "source": [
        "!unzip manifest-RbPGRCVv7392292744865323559.zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8P5VnssmUpS"
      },
      "source": [
        "%cd ../.."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UUZogBHmZT0"
      },
      "source": [
        "%cd /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtULIcU6mZmX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNnlW5b9mZ5d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1WB6QbcJHq4"
      },
      "source": [
        "!rm manifest-RbPGRCVv7392292744865323559.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIjnZYCfYfFY"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_u9I34pQN6-S"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci7zoLQAcGfK"
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQxzVzvGc-xl"
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YxIWD41dbFk"
      },
      "source": [
        "dicom_image_description = pd.read_csv(\"dicom_image_description.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwN-m576JUTs"
      },
      "source": [
        "!rm dicom_image_description.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJMMw99fpyqL"
      },
      "source": [
        "import pydicom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvs10XGxzUbc"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Bi1QkI_apWD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2t0mFaWaTK_"
      },
      "source": [
        "!rm convert(1).py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jcMJ0rH1rwe"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfKP9jr9dlWj"
      },
      "source": [
        "PAK = []\n",
        "PAKT= []\n",
        "\n",
        "with open('Patient_Detail.csv', 'w', newline ='') as csvfile:\n",
        "    \n",
        "\n",
        "    for root, dirs, files in os.walk(\".\"):\n",
        "      path = root.split(os.sep)\n",
        "      #print(os.sep)\n",
        "      #print(root)\n",
        "      pathr = str(root)\n",
        "      Q = re.search('T1', pathr)\n",
        "      W = re.search('PE', pathr)\n",
        "\n",
        "      if (not (Q or W)):\n",
        "        continue\n",
        "\n",
        "      #cnt = 0\n",
        "      #print((len(path) - 1) * '---', os.path.basename(root))\n",
        "      for file in files:\n",
        "        #print(len(path) * '---', file)\n",
        "        #print(path)\n",
        "        \n",
        "        image = str(file)\n",
        "        image2 = image\n",
        "        if image.endswith('.dcm'):\n",
        "          ds = dicom.dcmread(os.path.join(pathr, str(file)), force=True)\n",
        "          ds.file_meta.TransferSyntaxUID = pydicom.uid.ImplicitVRLittleEndian  # or whatever is the correct transfer syntax for the file\n",
        "          image = str(file)\n",
        "          rows = []\n",
        "          #print(image)\n",
        "          \n",
        "          pixel_array_numpy = ds.pixel_array\n",
        "          pansh = np.shape(pixel_array_numpy)\n",
        "          image = image.replace('.dcm', '.jpg')\n",
        "          #print(pansh)\n",
        "          if len(pansh)>2:\n",
        "            continue\n",
        "          pixel_array_numpy = np.reshape(pixel_array_numpy, tuple([1,pansh[0],pansh[1]]))\n",
        "          #print(np.shape(pixel_array_numpy))\n",
        "          #print(pixel_array_numpy)\n",
        "          pixel_array_numpy = np.transpose(pixel_array_numpy)\n",
        "          \n",
        "          PAK.append(pixel_array_numpy)\n",
        "          PAKT.append(os.path.join(pathr, image))\n",
        "          #fp = False\n",
        "\n",
        "          #else :  \n",
        "            #cv2.imwrite(os.path.join(pathr, image), pixel_array_numpy)\n",
        "            #fp = True\n",
        "          \n",
        "          #if n % 50 == 0:\n",
        "          #    print('{} image converted'.format(n))\n",
        "          '''\n",
        "          for field in fieldnames:\n",
        "              if ds.data_element(field) is None:\n",
        "                  rows.append('')\n",
        "              else:\n",
        "                  x = str(ds.data_element(field)).replace(\"'\", \"\")\n",
        "                  y = x.find(\":\")\n",
        "                  x = x[y+2:]\n",
        "                  rows.append(x)        \n",
        "          writer.writerow(rows)\n",
        "          '''\n",
        "          '''\n",
        "          for tag in fieldnames:\n",
        "            if tag not in ds:\n",
        "              writer.writerow('')\n",
        "              continue\n",
        "\n",
        "          elem = ds[tag]\n",
        "           $Parse elem however you wish, watch out for elements with a byte VR though!\n",
        "          value = elem.value\n",
        "          if isinstance(value, bytes):\n",
        "            value = \"Binary data of length {}\".format(len(elem))#.length)\n",
        "          row = \"{}, {}, {}\".format(elem.tag, elem.VR, value)\n",
        "          writer.writerow(row)\n",
        "          '''\n",
        "        if image2.endswith('.dcm'):\n",
        "          #print(\"DJB\")\n",
        "\n",
        "          os.remove(os.path.join(pathr, image2))\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOIpfV2rSGn0"
      },
      "source": [
        "print(len(PAK))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pgAigaxPcZ6"
      },
      "source": [
        "print(len(PAKT))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEpWaauNStq7"
      },
      "source": [
        "np.save('PAK.npy', PAK, allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukMYLXVWPhA-"
      },
      "source": [
        "np.save('PAKT.npy', PAKT, allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDQKsj1cPw6q"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPyPZfwBP2IC"
      },
      "source": [
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehMirLCyWoha"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b5R4TFrWtL3"
      },
      "source": [
        "!cp ./PAK.npy ./gdrive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m_kCUXPW16S"
      },
      "source": [
        "!cp ./PAKT.npy ./gdrive/MyDrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DmLQ6FGW9W5"
      },
      "source": [
        "%cd ./gdrive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8xrIe3EXBwm"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt6poXCnXGzv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmDQaWLNXG_G"
      },
      "source": [
        "print(PAKT[1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EagLtusB013v"
      },
      "source": [
        "!find . -name *.dcm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khgxwaINUFN-"
      },
      "source": [
        "!mv ./manifest-RbPGRCVv7392292744865323559 ./manifest-RbPGRCVv7392292744865323559X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw7_wAlpXl_v"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce5JbgU1YLTN"
      },
      "source": [
        "!rm ./manifest-RbPGRCVv7392292744865323559.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECT3PJUqXsyo"
      },
      "source": [
        "!zip -r  ./OA.zip ./manifest-RbPGRCVv7392292744865323559X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tbIXbUojRw6"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OajIC3RsjSsp"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBd6ZNf4cxT9"
      },
      "source": [
        "!ls -lh GX.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcQysiS-c6hy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ofb51XV3aeyM"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7cJAzYpYVHr"
      },
      "source": [
        "from google.colab import files as WF\n",
        "WF.download('GX.zip') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CJt0hpPZ65o"
      },
      "source": [
        "!unzip XA.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngqki1keaB72"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3viYT4juUI7u"
      },
      "source": [
        "%cd manifest-RbPGRCVv7392292744865323559"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFdZs-q_UX2K"
      },
      "source": [
        "%cd Breast-MRI-NACT-Pilot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U0qncYtUi38"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35MYrxDt8mFO"
      },
      "source": [
        "# traverse root directory, and list directories as dirs and files as files\n",
        "for root, dirs, files in os.walk(\".\"):\n",
        "    path = root.split(os.sep)\n",
        "    print((len(path) - 1) * '---', os.path.basename(root))\n",
        "    for file in files:\n",
        "        print(len(path) * '---', file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9Z_KCTFV-2j"
      },
      "source": [
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWKWCDKGWCZB"
      },
      "source": [
        "path = os.walk(\".\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9yM5iyJWZzA"
      },
      "source": [
        "print(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwcSQxZmWd-t"
      },
      "source": [
        "'''\n",
        "for directories in path:\n",
        "  for directory in directories:\n",
        "    print(directory)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33QZ6zirXEyX"
      },
      "source": [
        "dirs = []\n",
        "fils = []\n",
        "for root, directories, files in path:\n",
        "    for directory in directories:\n",
        "        dirs.append(directory)\n",
        "\n",
        "        print(directory)\n",
        "    for file in files:\n",
        "        fils.append(file)\n",
        "\n",
        "        print(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imT9rsHMYY9_"
      },
      "source": [
        "for i in dirs:\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzDw_vjn6Xks"
      },
      "source": [
        "!cd jpegfromdicom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ur_VxRk6gab"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTlUUWKxjNob"
      },
      "source": [
        "# Copyright (c) 2020 NVIDIA Corporation. All rights reserved.\n",
        "# This work is licensed under a NVIDIA Open Source Non-commercial license\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.modules.utils as utils\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "\n",
        "## Utilities\n",
        "@torch.jit.script\n",
        "def fuse_mul_add_mul(f, cell_states, i, g):\n",
        "    return f * cell_states + i * g\n",
        "\n",
        "def chkpt_blk(cc_i, cc_f, cc_o, cc_g, cell_states):\n",
        "    i = torch.sigmoid(cc_i)\n",
        "    f = torch.sigmoid(cc_f)\n",
        "    o = torch.sigmoid(cc_o)\n",
        "    g = torch.tanh(cc_g)\n",
        "    \n",
        "    cell_states = fuse_mul_add_mul(f, cell_states, i, g)\n",
        "    outputs = o * torch.tanh(cell_states)\n",
        "\n",
        "    return outputs, cell_states\n",
        "\n",
        "## Standard Convolutional-LSTM Module\n",
        "class ConvLSTMCell(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels, hidden_channels, kernel_size = 5, bias = True):\n",
        "        \"\"\"\n",
        "        Construction of convolutional-LSTM cell.\n",
        "        \n",
        "        Arguments:\n",
        "        ----------\n",
        "        (Hyper-parameters of input/output interfaces)\n",
        "        input_channels: int\n",
        "            Number of channels of the input tensor.\n",
        "        hidden_channels: int\n",
        "            Number of channels of the hidden/cell states.\n",
        "        (Hyper-parameters of the convolutional opeations)\n",
        "        kernel_size: int or (int, int)\n",
        "            Size of the (squared) convolutional kernel.\n",
        "            Note: If the size is a single scalar k, it will be mapped to (k, k)\n",
        "            default: 3\n",
        "        bias: bool\n",
        "            Whether or not to add the bias in each convolutional operation.\n",
        "            default: True\n",
        "        \"\"\"\n",
        "        super(ConvLSTMCell, self).__init__()\n",
        "\n",
        "        self.input_channels  = input_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "\n",
        "        kernel_size = utils._pair(kernel_size)\n",
        "        padding     = kernel_size[0] // 2, kernel_size[1] // 2\n",
        "\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels  = input_channels + hidden_channels, \n",
        "            out_channels = 4 * hidden_channels,\n",
        "            kernel_size = kernel_size, padding = padding, bias = bias)\n",
        "\n",
        "        # Note: hidden/cell states are not intialized in construction\n",
        "        self.hidden_states, self.cell_state = None, None\n",
        "\n",
        "    def initialize(self, inputs):\n",
        "        \"\"\"\n",
        "        Initialization of convolutional-LSTM cell.\n",
        "        \n",
        "        Arguments: \n",
        "        ----------\n",
        "        inputs: a 4-th order tensor of size \n",
        "            [batch_size, input_channels, input_height, input_width]\n",
        "            Input tensor of convolutional-LSTM cell.\n",
        "        \"\"\"\n",
        "        device = inputs.device # \"cpu\" or \"cuda\"\n",
        "        batch_size, _, height, width = inputs.size()\n",
        "\n",
        "        # initialize both hidden and cell states to all zeros\n",
        "        self.hidden_states = torch.zeros(batch_size, \n",
        "            self.hidden_channels, height, width, device = device)\n",
        "        self.cell_states   = torch.zeros(batch_size, \n",
        "            self.hidden_channels, height, width, device = device)\n",
        "\n",
        "    def forward(self, inputs, first_step = False, checkpointing = False):\n",
        "        \"\"\"\n",
        "        Computation of convolutional-LSTM cell.\n",
        "        \n",
        "        Arguments:\n",
        "        ----------\n",
        "        inputs: a 4-th order tensor of size \n",
        "            [batch_size, input_channels, height, width] \n",
        "            Input tensor to the convolutional-LSTM cell.\n",
        "        first_step: bool\n",
        "            Whether the tensor is the first step in the input sequence. \n",
        "            Note: If so, both hidden and cell states are intialized to zeros tensors.\n",
        "            default: False\n",
        "        checkpointing: bool\n",
        "            Whether to use the checkpointing technique to reduce memory expense.\n",
        "            default: True\n",
        "        \n",
        "        Returns:\n",
        "        --------\n",
        "        hidden_states: another 4-th order tensor of size \n",
        "            [batch_size, hidden_channels, height, width]\n",
        "            Hidden states (and outputs) of the convolutional-LSTM cell.\n",
        "        \"\"\"\n",
        "        if first_step: self.initialize(inputs)\n",
        "\n",
        "        concat_conv = self.conv(torch.cat([inputs, self.hidden_states], dim = 1))\n",
        "        cc_i, cc_f, cc_o, cc_g = torch.split(concat_conv, self.hidden_channels, dim = 1)\n",
        "\n",
        "        if checkpointing:\n",
        "            self.hidden_states, self.cell_states = checkpoint(chkpt_blk, cc_i, cc_f, cc_o, cc_g, self.cell_states)\n",
        "        else:\n",
        "            i = torch.sigmoid(cc_i)\n",
        "            f = torch.sigmoid(cc_f)\n",
        "            o = torch.sigmoid(cc_o)\n",
        "            g = torch.tanh(cc_g)\n",
        "    \n",
        "            self.cell_states = fuse_mul_add_mul(f, self.cell_states, i, g)\n",
        "            self.hidden_states = o * torch.tanh(self.cell_states)\n",
        "        \n",
        "        return self.hidden_states \n",
        "\n",
        "\n",
        "## Convolutional Tensor-Train LSTM Module\n",
        "class ConvTTLSTMCell(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "        # interface of the Conv-TT-LSTM \n",
        "        input_channels, hidden_channels,\n",
        "        # convolutional tensor-train network\n",
        "        order = 3, steps = 3, ranks = 8,\n",
        "        # convolutional operations\n",
        "        kernel_size = 5, bias = True):\n",
        "        \"\"\"\n",
        "        Initialization of convolutional tensor-train LSTM cell.\n",
        "        Arguments:\n",
        "        ----------\n",
        "        (Hyper-parameters of the input/output channels)\n",
        "        input_channels:  int\n",
        "            Number of input channels of the input tensor.\n",
        "        hidden_channels: int\n",
        "            Number of hidden/output channels of the output tensor.\n",
        "        Note: the number of hidden_channels is typically equal to the one of input_channels.\n",
        "        (Hyper-parameters of the convolutional tensor-train format)\n",
        "        order: int\n",
        "            The order of convolutional tensor-train format (i.e. the number of core tensors).\n",
        "            default: 3\n",
        "        steps: int\n",
        "            The total number of past steps used to compute the next step.\n",
        "            default: 3\n",
        "        ranks: int\n",
        "            The ranks of convolutional tensor-train format (where all ranks are assumed to be the same).\n",
        "            default: 8\n",
        "        (Hyper-parameters of the convolutional operations)\n",
        "        kernel_size: int or (int, int)\n",
        "            Size of the (squared) convolutional kernel.\n",
        "            Note: If the size is a single scalar k, it will be mapped to (k, k)\n",
        "            default: 5\n",
        "        bias: bool\n",
        "            Whether or not to add the bias in each convolutional operation.\n",
        "            default: True\n",
        "        \"\"\"\n",
        "        super(ConvTTLSTMCell, self).__init__()\n",
        "\n",
        "        ## Input/output interfaces\n",
        "        self.input_channels  = input_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "\n",
        "        ## Convolutional tensor-train network\n",
        "        self.steps = steps\n",
        "        self.order = order\n",
        "        self.lags  = steps - order + 1\n",
        "\n",
        "        ## Convolutional operations\n",
        "        kernel_size = utils._pair(kernel_size)\n",
        "        padding     = kernel_size[0] // 2, kernel_size[1] // 2\n",
        "\n",
        "        Conv2d = lambda in_channels, out_channels: nn.Conv2d(\n",
        "            in_channels = in_channels, out_channels = out_channels, \n",
        "            kernel_size = kernel_size, padding = padding, bias = bias)\n",
        "\n",
        "        ## Convolutional layers\n",
        "        self.layers  = nn.ModuleList()\n",
        "        self.layers_ = nn.ModuleList()\n",
        "        for l in range(order):\n",
        "            self.layers.append(Conv2d(\n",
        "                in_channels  = ranks if l < order - 1 else ranks + input_channels, \n",
        "                out_channels = ranks if l < order - 1 else 4 * hidden_channels))\n",
        "\n",
        "            self.layers_.append(Conv2d(\n",
        "                in_channels = self.lags * hidden_channels, out_channels = ranks))\n",
        "\n",
        "    def initialize(self, inputs):\n",
        "        \"\"\" \n",
        "        Initialization of the hidden/cell states of the convolutional tensor-train cell.\n",
        "        Arguments:\n",
        "        ----------\n",
        "        inputs: 4-th order tensor of size \n",
        "            [batch_size, input_channels, height, width]\n",
        "            Input tensor to the convolutional tensor-train LSTM cell.\n",
        "        \"\"\"\n",
        "        device = inputs.device # \"cpu\" or \"cuda\"\n",
        "        batch_size, _, height, width = inputs.size()\n",
        "\n",
        "        # initialize both hidden and cell states to all zeros\n",
        "        self.hidden_states  = [torch.zeros(batch_size, self.hidden_channels, \n",
        "            height, width, device = device) for t in range(self.steps)]\n",
        "        self.hidden_pointer = 0 # pointing to the position to be updated\n",
        "\n",
        "        self.cell_states = torch.zeros(batch_size, \n",
        "            self.hidden_channels, height, width, device = device)\n",
        "\n",
        "    def forward(self, inputs, first_step = False, checkpointing = False):\n",
        "        \"\"\"\n",
        "        Computation of the convolutional tensor-train LSTM cell.\n",
        "        \n",
        "        Arguments:\n",
        "        ----------\n",
        "        inputs: a 4-th order tensor of size \n",
        "            [batch_size, input_channels, height, width] \n",
        "            Input tensor to the convolutional-LSTM cell.\n",
        "        first_step: bool\n",
        "            Whether the tensor is the first step in the input sequence. \n",
        "            Note: If so, both hidden and cell states are intialized to zeros tensors.\n",
        "            default: False\n",
        "        checkpointing: bool\n",
        "            Whether to use the checkpointing technique to reduce memory expense.\n",
        "            default: True\n",
        "        \n",
        "        Returns:\n",
        "        --------\n",
        "        hidden_states: a list of 4-th order tensor of size \n",
        "            [batch_size, input_channels, height, width]\n",
        "            Hidden states (and outputs) of the convolutional-LSTM cell.\n",
        "        \"\"\"\n",
        "\n",
        "        if first_step: self.initialize(inputs) # intialize the states at the first step\n",
        "\n",
        "        ## (1) Convolutional tensor-train module\n",
        "        for l in range(self.order):\n",
        "            input_pointer = self.hidden_pointer if l == 0 else (input_pointer + 1) % self.steps\n",
        "\n",
        "            input_states = self.hidden_states[input_pointer:] + self.hidden_states[:input_pointer]\n",
        "            input_states = input_states[:self.lags]\n",
        "\n",
        "            input_states = torch.cat(input_states, dim = 1)\n",
        "            input_states = self.layers_[l](input_states)\n",
        "\n",
        "            if l == 0:\n",
        "                temp_states = input_states\n",
        "            else: # if l > 0:\n",
        "                temp_states = input_states + self.layers[l-1](temp_states)\n",
        "                \n",
        "        ## (2) Standard convolutional-LSTM module\n",
        "        concat_conv = self.layers[-1](torch.cat([inputs, temp_states], dim = 1))\n",
        "        cc_i, cc_f, cc_o, cc_g = torch.split(concat_conv, self.hidden_channels, dim = 1)\n",
        "\n",
        "        if checkpointing:\n",
        "            outputs, self.cell_states = checkpoint(chkpt_blk, cc_i, cc_f, cc_o, cc_g, self.cell_states)\n",
        "        else:\n",
        "            i = torch.sigmoid(cc_i)\n",
        "            f = torch.sigmoid(cc_f)\n",
        "            o = torch.sigmoid(cc_o)\n",
        "            g = torch.tanh(cc_g)\n",
        "    \n",
        "            self.cell_states = fuse_mul_add_mul(f, self.cell_states, i, g)\n",
        "            outputs = o * torch.tanh(self.cell_states)\n",
        "\n",
        "        self.hidden_states[self.hidden_pointer] = outputs\n",
        "        self.hidden_pointer = (self.hidden_pointer + 1) % self.steps\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vemWeqoljO9W"
      },
      "source": [
        "# Copyright (c) 2020 NVIDIA Corporation. All rights reserved.\n",
        "# This work is licensed under a NVIDIA Open Source Non-commercial license\n",
        "\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#######$$$$$$$$$$$$MMMMMM$$$$from utils.convlstmcell import ConvLSTMCell, ConvTTLSTMCell\n",
        "#sys.exit()\n",
        "## Convolutional-LSTM network\n",
        "class ConvLSTMNet(nn.Module):\n",
        "    def __init__(self,\n",
        "        # input to the model\n",
        "        input_channels,\n",
        "        # architecture of the model\n",
        "        layers_per_block, hidden_channels, skip_stride = None,\n",
        "        # parameters of convolutional tensor-train layers\n",
        "        cell = \"convlstm\", cell_params = {}, \n",
        "        # parameters of convolutional operation\n",
        "        kernel_size = 3, bias = True,\n",
        "        # output function and output format\n",
        "        output_sigmoid = False):\n",
        "        \"\"\"\n",
        "        Initialization of a Conv-LSTM network.\n",
        "        \n",
        "        Arguments:\n",
        "        ----------\n",
        "        (Hyper-parameters of input interface)\n",
        "        input_channels: int \n",
        "            The number of channels for input video.\n",
        "            Note: 3 for colored video, 1 for gray video. \n",
        "        (Hyper-parameters of model architecture)\n",
        "        layers_per_block: list of ints\n",
        "            Number of Conv-LSTM layers in each block. \n",
        "        hidden_channels: list of ints\n",
        "            Number of output channels.\n",
        "        Note: The length of hidden_channels (or layers_per_block) is equal to number of blocks.\n",
        "        skip_stride: int\n",
        "            The stride (in term of blocks) of the skip connections\n",
        "            default: None, i.e. no skip connection\n",
        "        \n",
        "        [cell_params: dictionary\n",
        "            order: int\n",
        "                The recurrent order of convolutional tensor-train cells.\n",
        "                default: 3\n",
        "            steps: int\n",
        "                The number of previous steps used in the recurrent cells.\n",
        "                default: 5\n",
        "            rank: int\n",
        "                The tensor-train rank of convolutional tensor-train cells.\n",
        "                default: 16\n",
        "        ]\n",
        "        \n",
        "        (Parameters of convolutional operations)\n",
        "        kernel_size: int or (int, int)\n",
        "            Size of the (squared) convolutional kernel.\n",
        "            default: 3\n",
        "        bias: bool \n",
        "            Whether to add bias in the convolutional operation.\n",
        "            default: True\n",
        "        (Parameters of the output function)\n",
        "        output_sigmoid: bool\n",
        "            Whether to apply sigmoid function after the output layer.\n",
        "            default: False\n",
        "        \"\"\"\n",
        "        super(ConvLSTMNet, self).__init__()\n",
        "\n",
        "        ## Hyperparameters\n",
        "        self.layers_per_block = layers_per_block\n",
        "        self.hidden_channels  = hidden_channels\n",
        "\n",
        "        self.num_blocks = len(layers_per_block)\n",
        "        assert self.num_blocks == len(hidden_channels), \"Invalid number of blocks.\"\n",
        "\n",
        "        self.skip_stride = (self.num_blocks + 1) if skip_stride is None else skip_stride\n",
        "\n",
        "        self.output_sigmoid = output_sigmoid\n",
        "\n",
        "        ## Module type of convolutional LSTM layers\n",
        "\n",
        "        if cell == \"convlstm\": # standard convolutional LSTM\n",
        "            Cell = lambda in_channels, out_channels: ConvLSTMCell(\n",
        "            input_channels = in_channels, hidden_channels = out_channels,\n",
        "            kernel_size = kernel_size, bias = bias)\n",
        "\n",
        "        elif cell == \"convttlstm\": # convolutional tensor-train LSTM\n",
        "            Cell = lambda in_channels, out_channels: ConvTTLSTMCell(\n",
        "                input_channels = in_channels, hidden_channels = out_channels,\n",
        "                order = cell_params[\"order\"], steps = cell_params[\"steps\"], ranks = cell_params[\"ranks\"], \n",
        "                kernel_size = kernel_size, bias = bias)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        ## Construction of convolutional tensor-train LSTM network\n",
        "\n",
        "        # stack the convolutional-LSTM layers with skip connections \n",
        "        self.layers = nn.ModuleDict()\n",
        "        for b in range(self.num_blocks):\n",
        "            for l in range(layers_per_block[b]):\n",
        "                # number of input channels to the current layer\n",
        "                if l > 0: \n",
        "                    channels = hidden_channels[b]\n",
        "                elif b == 0: # if l == 0 and b == 0:\n",
        "                    channels = input_channels\n",
        "                else: # if l == 0 and b > 0:\n",
        "                    channels = hidden_channels[b-1]\n",
        "                    if b > self.skip_stride:\n",
        "                        channels += hidden_channels[b-1-self.skip_stride] \n",
        "\n",
        "                lid = \"b{}l{}\".format(b, l) # layer ID\n",
        "                self.layers[lid] = Cell(channels, hidden_channels[b])\n",
        "\n",
        "        # number of input channels to the last layer (output layer)\n",
        "        channels = hidden_channels[-1]\n",
        "        if self.num_blocks >= self.skip_stride:\n",
        "            channels += hidden_channels[-1-self.skip_stride]\n",
        "\n",
        "        self.layers[\"output\"] = nn.Conv2d(channels, input_channels, \n",
        "            kernel_size = 1, padding = 0, bias = True)\n",
        "\n",
        "\n",
        "    def forward(self, inputs, input_frames, future_frames, output_frames, \n",
        "        teacher_forcing = False, scheduled_sampling_ratio = 0, checkpointing = False):\n",
        "        \"\"\"\n",
        "        Computation of Convolutional LSTM network.\n",
        "        \n",
        "        Arguments:\n",
        "        ----------\n",
        "        inputs: a 5-th order tensor of size [batch_size, input_frames, input_channels, height, width] \n",
        "            Input tensor (video) to the deep Conv-LSTM network. \n",
        "        \n",
        "        input_frames: int\n",
        "            The number of input frames to the model.\n",
        "        future_frames: int\n",
        "            The number of future frames predicted by the model.\n",
        "        output_frames: int\n",
        "            The number of output frames returned by the model.\n",
        "        teacher_forcing: bool\n",
        "            Whether the model is trained in teacher_forcing mode.\n",
        "            Note 1: In test mode, teacher_forcing should be set as False.\n",
        "            Note 2: If teacher_forcing mode is on,  # of frames in inputs = total_steps\n",
        "                    If teacher_forcing mode is off, # of frames in inputs = input_frames\n",
        "        scheduled_sampling_ratio: float between [0, 1]\n",
        "            The ratio of ground-truth frames used in teacher_forcing mode.\n",
        "            default: 0 (i.e. no teacher forcing effectively)\n",
        "        Returns:\n",
        "        --------\n",
        "        outputs: a 5-th order tensor of size [batch_size, output_frames, hidden_channels, height, width]\n",
        "            Output frames of the convolutional-LSTM module.\n",
        "        \"\"\"\n",
        "\n",
        "        # compute the teacher forcing mask \n",
        "        if teacher_forcing and scheduled_sampling_ratio > 1e-6:\n",
        "            # generate the teacher_forcing mask (4-th order)\n",
        "            teacher_forcing_mask = torch.bernoulli(scheduled_sampling_ratio * \n",
        "                torch.ones(inputs.size(0), future_frames - 1, 1, 1, 1, device = inputs.device))\n",
        "        else: # if not teacher_forcing or scheduled_sampling_ratio < 1e-6:\n",
        "            teacher_forcing = False\n",
        "\n",
        "        # the number of time steps in the computational graph\n",
        "        total_steps = input_frames + future_frames - 1\n",
        "        outputs = [None] * total_steps\n",
        "\n",
        "        for t in range(total_steps):\n",
        "            # input_: 4-th order tensor of size [batch_size, input_channels, height, width]\n",
        "            if t < input_frames: \n",
        "                input_ = inputs[:, t]\n",
        "            elif not teacher_forcing:\n",
        "                input_ = outputs[t-1]\n",
        "            else: # if t >= input_frames and teacher_forcing:\n",
        "                mask = teacher_forcing_mask[:, t - input_frames]\n",
        "                input_ = inputs[:, t] * mask + outputs[t-1] * (1 - mask)\n",
        "\n",
        "            queue = [] # previous outputs for skip connection\n",
        "            for b in range(self.num_blocks):\n",
        "                for l in range(self.layers_per_block[b]):\n",
        "                    lid = \"b{}l{}\".format(b, l) # layer ID\n",
        "                    input_ = self.layers[lid](input_, \n",
        "                        first_step = (t == 0), checkpointing = checkpointing)\n",
        "\n",
        "                queue.append(input_)\n",
        "                if b >= self.skip_stride:\n",
        "                    input_ = torch.cat([input_, queue.pop(0)], dim = 1) # concat over the channels\n",
        "\n",
        "            # map the hidden states to predictive frames (with optional sigmoid function)\n",
        "            outputs[t] = self.layers[\"output\"](input_)\n",
        "            if self.output_sigmoid:\n",
        "                outputs[t] = torch.sigmoid(outputs[t])\n",
        "\n",
        "        # return the last output_frames of the outputs\n",
        "        outputs = outputs[-output_frames:]\n",
        "\n",
        "        # 5-th order tensor of size [batch_size, output_frames, channels, height, width]\n",
        "        outputs = torch.stack([outputs[t] for t in range(output_frames)], dim = 1)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frRL5EUrhTs4"
      },
      "source": [
        "!git clone https://github.com/richzhang/PerceptualSimilarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MLcLlc9hYV_"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUt-hkW_hatx"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bPf2WI0hqPB"
      },
      "source": [
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WN0X4eFEh3YJ"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Glb1tiafiHxJ"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jIIqSGqiNF-"
      },
      "source": [
        "%cd PerceptualSimilarity/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cr3j-wgAiR-t"
      },
      "source": [
        "%cd /content/PerceptualSimilarity/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO4-wL5si--x"
      },
      "source": [
        "!touch __init__.py\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqmYosvXjKwk"
      },
      "source": [
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBcpTAc2m2jh"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cW60FKnQmD_n"
      },
      "source": [
        "# Copyright (c) 2020 NVIDIA Corporation. All rights reserved.\n",
        "# This work is licensed under a NVIDIA Open Source Non-commercial license\n",
        "\n",
        "# system modules\n",
        "import os, argparse\n",
        "\n",
        "# basic pytorch modules\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# computer vision/image processing modules \n",
        "import torchvision\n",
        "import skimage.metrics\n",
        "\n",
        "# math/probability modules\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# custom utilities\n",
        "###########$$$$$$$$$MMMMMM$$$$$$$$$$###from utils.convlstmnet import ConvLSTMNet \n",
        "#####$$$$$$$MMMMM$$$$$######from dataloader import KTH_Dataset, MNIST_Dataset\n",
        "\n",
        "#####$$$$MMMM$$$$$#########from utils.gpu_affinity import set_affinity\n",
        "\n",
        "# perceptive quality\n",
        "#import PerceptualSimilarity\n",
        "import PerceptualSimilarity.models as PSmodels\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1Ljc0QpjtoF"
      },
      "source": [
        "'''\n",
        "# Copyright (c) 2020 NVIDIA Corporation. All rights reserved.\n",
        "# This work is licensed under a NVIDIA Open Source Non-commercial license\n",
        "\n",
        "# system modules\n",
        "import os, argparse\n",
        "\n",
        "# basic pytorch modules\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# computer vision/image processing modules \n",
        "import torchvision\n",
        "import skimage.metrics\n",
        "\n",
        "# math/probability modules\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# custom utilities\n",
        "###########$$$$$$$$$MMMMMM$$$$$$$$$$###from utils.convlstmnet import ConvLSTMNet \n",
        "#####$$$$$$$MMMMM$$$$$######from dataloader import KTH_Dataset, MNIST_Dataset\n",
        "\n",
        "#####$$$$MMMM$$$$$#########from utils.gpu_affinity import set_affinity\n",
        "\n",
        "# perceptive quality\n",
        "import PerceptualSimilarity\n",
        "#import PerceptualSimilarity.models as PSmodels\n",
        "'''\n",
        "\n",
        "def main(args):\n",
        "    ## Distributed computing\n",
        "\n",
        "    # utility for synchronization\n",
        "    def reduce_tensor(tensor):\n",
        "        rt = tensor.clone()\n",
        "        torch.distributed.all_reduce(rt, op = torch.distributed.ReduceOp.SUM)\n",
        "        return rt\n",
        "\n",
        "    # enable distributed computing\n",
        "    if args.distributed:\n",
        "        set_affinity(args.local_rank)\n",
        "        num_devices = torch.cuda.device_count()\n",
        "        torch.cuda.set_device(args.local_rank)\n",
        "        torch.distributed.init_process_group(backend = 'nccl', init_method = 'env://')\n",
        "\n",
        "        world_size  = torch.distributed.get_world_size() #os.environ['WORLD_SIZE']\n",
        "        print('num_devices', num_devices, \n",
        "              'local_rank', args.local_rank, \n",
        "              'world_size', world_size)\n",
        "    else: # if not args.distributed:\n",
        "        num_devices, world_size = 1, 1\n",
        "\n",
        "    ## Model preparation (Conv-LSTM or Conv-TT-LSTM)\n",
        "\n",
        "    # construct the model with the specified hyper-parameters\n",
        "    model = ConvLSTMNet(\n",
        "        input_channels = args.img_channels, \n",
        "        output_sigmoid = args.use_sigmoid,\n",
        "        # model architecture\n",
        "        layers_per_block = (3, 3, 3, 3), \n",
        "        hidden_channels  = (32, 48, 48, 32), \n",
        "        skip_stride = 2,\n",
        "        # convolutional tensor-train layers\n",
        "        cell = args.model,\n",
        "        cell_params = {\n",
        "            \"order\": args.model_order, \n",
        "            \"steps\": args.model_steps, \n",
        "            \"ranks\": args.model_ranks},\n",
        "        # convolutional parameters\n",
        "        kernel_size = args.kernel_size).cuda()\n",
        "\n",
        "    if args.distributed:\n",
        "        if args.use_apex: # use DDP from apex.parallel\n",
        "            from apex.parallel import DistributedDataParallel as DDP\n",
        "            model = DDP(model, delay_allreduce = True)\n",
        "        else: # use DDP from nn.parallel\n",
        "            from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "            model = DDP(model, device_ids = [args.local_rank])\n",
        "\n",
        "    PSmodel = PSmodels.PerceptualLoss(model = 'net-lin', \n",
        "        net = 'alex', use_gpu = True, gpu_ids = [args.local_rank])\n",
        "\n",
        "    ## Dataset Preparation (KTH, UCF, tinyUCF)\n",
        "    Dataset = {\"KTH\": KTH_Dataset, \"MNIST\": MNIST_Dataset}[args.dataset]\n",
        "\n",
        "    DATA_DIR = os.path.join(\"../data\", \n",
        "        {\"MNIST\": \"mnist\", \"KTH\": \"kth\"}[args.dataset])\n",
        "\n",
        "    # batch size for each process\n",
        "    total_batch_size  = args.batch_size\n",
        "    assert total_batch_size % world_size == 0, \\\n",
        "        'The batch_size is not divisible by world_size.'\n",
        "    batch_size = total_batch_size // world_size\n",
        "\n",
        "    total_frames = args.input_frames + args.future_frames\n",
        "\n",
        "    # dataloaer for the valiation dataset \n",
        "    test_data_path = os.path.join(DATA_DIR, args.test_data_file)\n",
        "    assert os.path.exists(test_data_path), \\\n",
        "        \"The test dataset does not exist. \"+test_data_path\n",
        "\n",
        "    test_dataset = Dataset({\"path\": test_data_path, \n",
        "        \"unique_mode\": True, \"num_frames\": total_frames, \"num_samples\": args.test_samples,\n",
        "        \"height\": args.img_height, \"width\": args.img_width, \"channels\": args.img_channels, 'training': False})\n",
        "\n",
        "    test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        test_dataset, num_replicas = world_size, rank = args.local_rank, shuffle = False)\n",
        "    test_loader  = torch.utils.data.DataLoader(\n",
        "        test_dataset, batch_size = batch_size, drop_last = True, \n",
        "        num_workers = num_devices * 4, pin_memory = True, sampler = test_sampler)\n",
        "\n",
        "    test_samples = len(test_loader) * total_batch_size\n",
        "    print(test_samples)\n",
        "\n",
        "    ## Main script for test phase \n",
        "    MSE_  = torch.zeros((args.future_frames), dtype = torch.float32).cuda()\n",
        "    PSNR_ = torch.zeros((args.future_frames), dtype = torch.float32).cuda()\n",
        "    SSIM_ = torch.zeros((args.future_frames), dtype = torch.float32).cuda()\n",
        "    PIPS_ = torch.zeros((args.future_frames), dtype = torch.float32).cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "\n",
        "        for it, frames in enumerate(test_loader):\n",
        "\n",
        "            frames = frames.permute(0, 1, 4, 2, 3).cuda()\n",
        "            inputs = frames[:,  :args.input_frames]\n",
        "            origin = frames[:, -args.future_frames:]\n",
        "\n",
        "            pred = model(inputs, \n",
        "                input_frames  =  args.input_frames, \n",
        "                future_frames = args.future_frames, \n",
        "                output_frames = args.future_frames, \n",
        "                teacher_forcing = False)\n",
        "\n",
        "            # accumlate the statistics per frame\n",
        "            for t in range(-args.future_frames, 0):\n",
        "                origin_, pred_ = origin[:, t], pred[:, t]\n",
        "\n",
        "                if args.img_channels == 1:\n",
        "                    origin_ = origin_.repeat([1, 3, 1, 1])\n",
        "                    pred_   =   pred_.repeat([1, 3, 1, 1])\n",
        "\n",
        "                dist = PSmodel(origin_, pred_)\n",
        "                PIPS_[t] += torch.sum(dist).item()\n",
        "\n",
        "            origin = origin.permute(0, 1, 3, 4, 2).cpu().numpy()\n",
        "            pred   =   pred.permute(0, 1, 3, 4, 2).cpu().numpy()\n",
        "\n",
        "            for t in range(-args.future_frames, 0):\n",
        "                for i in range(batch_size):\n",
        "                    origin_, pred_ = origin[i, t], pred[i, t]\n",
        "\n",
        "                    if args.img_channels == 1:\n",
        "                        origin_ = np.squeeze(origin_, axis = -1)\n",
        "                        pred_   = np.squeeze(pred_,   axis = -1)\n",
        "\n",
        "                    MSE_[t]  += skimage.metrics.mean_squared_error(origin_, pred_)\n",
        "                    PSNR_[t] += skimage.metrics.peak_signal_noise_ratio(origin_, pred_)\n",
        "                    SSIM_[t] += skimage.metrics.structural_similarity(origin_, pred_, multichannel = args.img_channels > 1)\n",
        "\n",
        "        if args.distributed:\n",
        "            MSE  = reduce_tensor( MSE_) / test_samples\n",
        "            PSNR = reduce_tensor(PSNR_) / test_samples\n",
        "            SSIM = reduce_tensor(SSIM_) / test_samples\n",
        "            PIPS = reduce_tensor(PIPS_) / test_samples\n",
        "        else: # if not args.distributed:\n",
        "            MSE  = MSE_  / test_samples\n",
        "            PSNR = PSNR_ / test_samples\n",
        "            SSIM = SSIM_ / test_samples\n",
        "            PIPS = PIPS_ / test_samples\n",
        "\n",
        "    if args.local_rank == 0:\n",
        "        print(\"MSE: {} (x1e-3)\\nPSNR: {}\\nSSIM: {}\\nLPIPS: {}\".format(\n",
        "            1e3 * torch.mean(MSE).cpu().item(), torch.mean(PSNR).cpu().item(), \n",
        "            torch.mean(SSIM).cpu().item(), torch.mean(PIPS).cpu().item()))\n",
        "\n",
        "    print( \"MSE:\",  MSE.cpu().numpy())\n",
        "    print(\"PSNR:\", PSNR.cpu().numpy())\n",
        "    print(\"SSIM:\", SSIM.cpu().numpy())\n",
        "    print(\"PIPS:\", PIPS.cpu().numpy())\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description = \"Conv-TT-LSTM Test\")\n",
        "\n",
        "    ## Devices (Single GPU / Distributed computing)\n",
        "\n",
        "    # whether to use distributed computing\n",
        "    parser.add_argument('--use-distributed', dest = \"distributed\", \n",
        "        action = 'store_true',  help = 'Use distributed computing in testing.')\n",
        "    parser.add_argument( '--no-distributed', dest = \"distributed\", \n",
        "        action = 'store_false', help = 'Use single process (GPU) in testing.')\n",
        "    parser.set_defaults(distributed = True)\n",
        "\n",
        "    parser.add_argument('--use-apex', dest = 'use_apex', \n",
        "        action = 'store_true',  help = 'Use apex.parallel.')\n",
        "    parser.add_argument( '--no-apex', dest = 'use_apex', \n",
        "        action = 'store_false', help = 'Use torch.nn.distributed.')\n",
        "    parser.set_defaults(use_apex = False)\n",
        "\n",
        "    # arguments for distributed computing \n",
        "    parser.add_argument('--local_rank', default = 0, type = int)\n",
        "\n",
        "    ## Data format (batch_size x time_steps x height x width x channels)\n",
        "\n",
        "    # batch size (0) \n",
        "    parser.add_argument('--batch-size', default = 16, type = int,\n",
        "        help = 'The total batch size in each test iteration.')\n",
        "\n",
        "    # frame split (1)\n",
        "    parser.add_argument('--input-frames',  default = 10, type = int,\n",
        "        help = 'The number of input frames to the model.')\n",
        "    parser.add_argument('--future-frames', default = 10, type = int,\n",
        "        help = 'The number of predicted frames of the model.')\n",
        "\n",
        "    # frame format (2, 3, 4)\n",
        "    parser.add_argument('--img-channels', default =  3, type = int, \n",
        "        help = 'The number of channels in each video frame.')\n",
        "\n",
        "    parser.add_argument('--img-height',   default = 120, type = int, \n",
        "        help = 'The image height of each video frame.')\n",
        "    parser.add_argument('--img-width',    default = 120, type = int, \n",
        "        help = 'The image width  of each video frame.')\n",
        "\n",
        "    ## Models (Conv-LSTM or Conv-TT-LSTM)\n",
        "\n",
        "    # model type\n",
        "    parser.add_argument('--model', default = 'convlstm', type = str,\n",
        "        help = 'The model is either \\\"convlstm\\\"\" or \\\"convttlstm\\\".')\n",
        "    parser.add_argument('--checkpoint', default = \"checkpoint.pt\", type = str,\n",
        "        help = 'The name for the checkpoint.')\n",
        "\n",
        "    # output transformation\n",
        "    parser.add_argument('--use-sigmoid', dest = 'use_sigmoid',\n",
        "        action = 'store_true',  help = 'Use sigmoid function at the output of the model.')\n",
        "    parser.add_argument('--no-sigmoid',  dest = 'use_sigmoid', \n",
        "        action = 'store_false', help = 'Use output from the last layer as the final output.')\n",
        "    parser.set_defaults(use_sigmoid = False)\n",
        "\n",
        "    # parameters of the convolutional tensor-train layers\n",
        "    parser.add_argument('--model-order', default = 3, type = int, \n",
        "        help = 'The order of the convolutional tensor-train LSTMs.')\n",
        "    parser.add_argument('--model-steps', default = 3, type = int, \n",
        "        help = 'The steps of the convolutional tensor-train LSTMs')\n",
        "    parser.add_argument('--model-ranks',  default = 8, type = int, \n",
        "        help = 'The tensor rank of the convolutional tensor-train LSTMs.')\n",
        "    \n",
        "    # parameters of the convolutional operations\n",
        "    parser.add_argument('--kernel-size', default = 5, type = int,\n",
        "        help = \"The kernel size of the convolutional operations.\")\n",
        "\n",
        "    ## Dataset (Input)\n",
        "    parser.add_argument('--dataset', default = \"KTH\", type = str,\n",
        "        help = 'The dataset name. (Options: KTH, MNIST)')\n",
        "\n",
        "    parser.add_argument('--test-data-file', default = 'test', type = str, \n",
        "        help = 'Name of the folder/file for test set.')\n",
        "    parser.add_argument('--test-samples', default = 5000, type = int, \n",
        "        help = 'Number of samples in test dataset.')\n",
        "\n",
        "    main(parser.parse_args())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL7WNJanlUA_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXdq30OOj7D0"
      },
      "source": [
        "# Copyright (c) 2020 NVIDIA Corporation. All rights reserved.\n",
        "# This work is licensed under a NVIDIA Open Source Non-commercial license\n",
        "\n",
        "# system modules\n",
        "import os, argparse\n",
        "\n",
        "# basic pytorch modules\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# computer vision/image processing modules \n",
        "import torchvision\n",
        "import skimage.metrics\n",
        "\n",
        "# math/probability modules\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# custom utilities\n",
        "from utils.convlstmnet import ConvLSTMNet \n",
        "from dataloader import KTH_Dataset, MNIST_Dataset\n",
        "\n",
        "from utils.gpu_affinity import set_affinity\n",
        "\n",
        "# perceptive quality\n",
        "import PerceptualSimilarity.models as PSmodels\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    ## Distributed computing\n",
        "\n",
        "    # utility for synchronization\n",
        "    def reduce_tensor(tensor):\n",
        "        rt = tensor.clone()\n",
        "        torch.distributed.all_reduce(rt, op = torch.distributed.ReduceOp.SUM)\n",
        "        return rt\n",
        "\n",
        "    # enable distributed computing\n",
        "    if args.distributed:\n",
        "        set_affinity(args.local_rank)\n",
        "        num_devices = torch.cuda.device_count()\n",
        "        torch.cuda.set_device(args.local_rank)\n",
        "        torch.distributed.init_process_group(backend = 'nccl', init_method = 'env://')\n",
        "\n",
        "        world_size  = torch.distributed.get_world_size() #os.environ['WORLD_SIZE']\n",
        "        print('num_devices', num_devices, \n",
        "              'local_rank', args.local_rank, \n",
        "              'world_size', world_size)\n",
        "    else: # if not args.distributed:\n",
        "        num_devices, world_size = 1, 1\n",
        "\n",
        "    ## Model preparation (Conv-LSTM or Conv-TT-LSTM)\n",
        "\n",
        "    # construct the model with the specified hyper-parameters\n",
        "    model = ConvLSTMNet(\n",
        "        input_channels = args.img_channels, \n",
        "        output_sigmoid = args.use_sigmoid,\n",
        "        # model architecture\n",
        "        layers_per_block = (3, 3, 3, 3), \n",
        "        hidden_channels  = (32, 48, 48, 32), \n",
        "        skip_stride = 2,\n",
        "        # convolutional tensor-train layers\n",
        "        cell = args.model,\n",
        "        cell_params = {\n",
        "            \"order\": args.model_order, \n",
        "            \"steps\": args.model_steps, \n",
        "            \"ranks\": args.model_ranks},\n",
        "        # convolutional parameters\n",
        "        kernel_size = args.kernel_size).cuda()\n",
        "\n",
        "    if args.distributed:\n",
        "        if args.use_apex: # use DDP from apex.parallel\n",
        "            from apex.parallel import DistributedDataParallel as DDP\n",
        "            model = DDP(model, delay_allreduce = True)\n",
        "        else: # use DDP from nn.parallel\n",
        "            from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "            model = DDP(model, device_ids = [args.local_rank])\n",
        "\n",
        "    PSmodel = PSmodels.PerceptualLoss(model = 'net-lin', \n",
        "        net = 'alex', use_gpu = True, gpu_ids = [args.local_rank])\n",
        "\n",
        "    ## Dataset Preparation (KTH, UCF, tinyUCF)\n",
        "    Dataset = {\"KTH\": KTH_Dataset, \"MNIST\": MNIST_Dataset}[args.dataset]\n",
        "\n",
        "    DATA_DIR = os.path.join(\"../data\", \n",
        "        {\"MNIST\": \"mnist\", \"KTH\": \"kth\"}[args.dataset])\n",
        "\n",
        "    # batch size for each process\n",
        "    total_batch_size  = args.batch_size\n",
        "    assert total_batch_size % world_size == 0, \\\n",
        "        'The batch_size is not divisible by world_size.'\n",
        "    batch_size = total_batch_size // world_size\n",
        "\n",
        "    total_frames = args.input_frames + args.future_frames\n",
        "\n",
        "    # dataloaer for the valiation dataset \n",
        "    test_data_path = os.path.join(DATA_DIR, args.test_data_file)\n",
        "    assert os.path.exists(test_data_path), \\\n",
        "        \"The test dataset does not exist. \"+test_data_path\n",
        "\n",
        "    test_dataset = Dataset({\"path\": test_data_path, \n",
        "        \"unique_mode\": True, \"num_frames\": total_frames, \"num_samples\": args.test_samples,\n",
        "        \"height\": args.img_height, \"width\": args.img_width, \"channels\": args.img_channels, 'training': False})\n",
        "\n",
        "    test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        test_dataset, num_replicas = world_size, rank = args.local_rank, shuffle = False)\n",
        "    test_loader  = torch.utils.data.DataLoader(\n",
        "        test_dataset, batch_size = batch_size, drop_last = True, \n",
        "        num_workers = num_devices * 4, pin_memory = True, sampler = test_sampler)\n",
        "\n",
        "    test_samples = len(test_loader) * total_batch_size\n",
        "    print(test_samples)\n",
        "\n",
        "    ## Main script for test phase \n",
        "    MSE_  = torch.zeros((args.future_frames), dtype = torch.float32).cuda()\n",
        "    PSNR_ = torch.zeros((args.future_frames), dtype = torch.float32).cuda()\n",
        "    SSIM_ = torch.zeros((args.future_frames), dtype = torch.float32).cuda()\n",
        "    PIPS_ = torch.zeros((args.future_frames), dtype = torch.float32).cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "\n",
        "        for it, frames in enumerate(test_loader):\n",
        "\n",
        "            frames = frames.permute(0, 1, 4, 2, 3).cuda()\n",
        "            inputs = frames[:,  :args.input_frames]\n",
        "            origin = frames[:, -args.future_frames:]\n",
        "\n",
        "            pred = model(inputs, \n",
        "                input_frames  =  args.input_frames, \n",
        "                future_frames = args.future_frames, \n",
        "                output_frames = args.future_frames, \n",
        "                teacher_forcing = False)\n",
        "\n",
        "            # accumlate the statistics per frame\n",
        "            for t in range(-args.future_frames, 0):\n",
        "                origin_, pred_ = origin[:, t], pred[:, t]\n",
        "\n",
        "                if args.img_channels == 1:\n",
        "                    origin_ = origin_.repeat([1, 3, 1, 1])\n",
        "                    pred_   =   pred_.repeat([1, 3, 1, 1])\n",
        "\n",
        "                dist = PSmodel(origin_, pred_)\n",
        "                PIPS_[t] += torch.sum(dist).item()\n",
        "\n",
        "            origin = origin.permute(0, 1, 3, 4, 2).cpu().numpy()\n",
        "            pred   =   pred.permute(0, 1, 3, 4, 2).cpu().numpy()\n",
        "\n",
        "            for t in range(-args.future_frames, 0):\n",
        "                for i in range(batch_size):\n",
        "                    origin_, pred_ = origin[i, t], pred[i, t]\n",
        "\n",
        "                    if args.img_channels == 1:\n",
        "                        origin_ = np.squeeze(origin_, axis = -1)\n",
        "                        pred_   = np.squeeze(pred_,   axis = -1)\n",
        "\n",
        "                    MSE_[t]  += skimage.metrics.mean_squared_error(origin_, pred_)\n",
        "                    PSNR_[t] += skimage.metrics.peak_signal_noise_ratio(origin_, pred_)\n",
        "                    SSIM_[t] += skimage.metrics.structural_similarity(origin_, pred_, multichannel = args.img_channels > 1)\n",
        "\n",
        "        if args.distributed:\n",
        "            MSE  = reduce_tensor( MSE_) / test_samples\n",
        "            PSNR = reduce_tensor(PSNR_) / test_samples\n",
        "            SSIM = reduce_tensor(SSIM_) / test_samples\n",
        "            PIPS = reduce_tensor(PIPS_) / test_samples\n",
        "        else: # if not args.distributed:\n",
        "            MSE  = MSE_  / test_samples\n",
        "            PSNR = PSNR_ / test_samples\n",
        "            SSIM = SSIM_ / test_samples\n",
        "            PIPS = PIPS_ / test_samples\n",
        "\n",
        "    if args.local_rank == 0:\n",
        "        print(\"MSE: {} (x1e-3)\\nPSNR: {}\\nSSIM: {}\\nLPIPS: {}\".format(\n",
        "            1e3 * torch.mean(MSE).cpu().item(), torch.mean(PSNR).cpu().item(), \n",
        "            torch.mean(SSIM).cpu().item(), torch.mean(PIPS).cpu().item()))\n",
        "\n",
        "    print( \"MSE:\",  MSE.cpu().numpy())\n",
        "    print(\"PSNR:\", PSNR.cpu().numpy())\n",
        "    print(\"SSIM:\", SSIM.cpu().numpy())\n",
        "    print(\"PIPS:\", PIPS.cpu().numpy())\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description = \"Conv-TT-LSTM Test\")\n",
        "\n",
        "    ## Devices (Single GPU / Distributed computing)\n",
        "\n",
        "    # whether to use distributed computing\n",
        "    parser.add_argument('--use-distributed', dest = \"distributed\", \n",
        "        action = 'store_true',  help = 'Use distributed computing in testing.')\n",
        "    parser.add_argument( '--no-distributed', dest = \"distributed\", \n",
        "        action = 'store_false', help = 'Use single process (GPU) in testing.')\n",
        "    parser.set_defaults(distributed = True)\n",
        "\n",
        "    parser.add_argument('--use-apex', dest = 'use_apex', \n",
        "        action = 'store_true',  help = 'Use apex.parallel.')\n",
        "    parser.add_argument( '--no-apex', dest = 'use_apex', \n",
        "        action = 'store_false', help = 'Use torch.nn.distributed.')\n",
        "    parser.set_defaults(use_apex = False)\n",
        "\n",
        "    # arguments for distributed computing \n",
        "    parser.add_argument('--local_rank', default = 0, type = int)\n",
        "\n",
        "    ## Data format (batch_size x time_steps x height x width x channels)\n",
        "\n",
        "    # batch size (0) \n",
        "    parser.add_argument('--batch-size', default = 16, type = int,\n",
        "        help = 'The total batch size in each test iteration.')\n",
        "\n",
        "    # frame split (1)\n",
        "    parser.add_argument('--input-frames',  default = 10, type = int,\n",
        "        help = 'The number of input frames to the model.')\n",
        "    parser.add_argument('--future-frames', default = 10, type = int,\n",
        "        help = 'The number of predicted frames of the model.')\n",
        "\n",
        "    # frame format (2, 3, 4)\n",
        "    parser.add_argument('--img-channels', default =  3, type = int, \n",
        "        help = 'The number of channels in each video frame.')\n",
        "\n",
        "    parser.add_argument('--img-height',   default = 120, type = int, \n",
        "        help = 'The image height of each video frame.')\n",
        "    parser.add_argument('--img-width',    default = 120, type = int, \n",
        "        help = 'The image width  of each video frame.')\n",
        "\n",
        "    ## Models (Conv-LSTM or Conv-TT-LSTM)\n",
        "\n",
        "    # model type\n",
        "    parser.add_argument('--model', default = 'convlstm', type = str,\n",
        "        help = 'The model is either \\\"convlstm\\\"\" or \\\"convttlstm\\\".')\n",
        "    parser.add_argument('--checkpoint', default = \"checkpoint.pt\", type = str,\n",
        "        help = 'The name for the checkpoint.')\n",
        "\n",
        "    # output transformation\n",
        "    parser.add_argument('--use-sigmoid', dest = 'use_sigmoid',\n",
        "        action = 'store_true',  help = 'Use sigmoid function at the output of the model.')\n",
        "    parser.add_argument('--no-sigmoid',  dest = 'use_sigmoid', \n",
        "        action = 'store_false', help = 'Use output from the last layer as the final output.')\n",
        "    parser.set_defaults(use_sigmoid = False)\n",
        "\n",
        "    # parameters of the convolutional tensor-train layers\n",
        "    parser.add_argument('--model-order', default = 3, type = int, \n",
        "        help = 'The order of the convolutional tensor-train LSTMs.')\n",
        "    parser.add_argument('--model-steps', default = 3, type = int, \n",
        "        help = 'The steps of the convolutional tensor-train LSTMs')\n",
        "    parser.add_argument('--model-ranks',  default = 8, type = int, \n",
        "        help = 'The tensor rank of the convolutional tensor-train LSTMs.')\n",
        "    \n",
        "    # parameters of the convolutional operations\n",
        "    parser.add_argument('--kernel-size', default = 5, type = int,\n",
        "        help = \"The kernel size of the convolutional operations.\")\n",
        "\n",
        "    ## Dataset (Input)\n",
        "    parser.add_argument('--dataset', default = \"KTH\", type = str,\n",
        "        help = 'The dataset name. (Options: KTH, MNIST)')\n",
        "\n",
        "    parser.add_argument('--test-data-file', default = 'test', type = str, \n",
        "        help = 'Name of the folder/file for test set.')\n",
        "    parser.add_argument('--test-samples', default = 5000, type = int, \n",
        "        help = 'Number of samples in test dataset.')\n",
        "\n",
        "    main(parser.parse_args())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}